# Assignment 4

## 1. Goal
The goal of this assignment is to investigate the optimal settings for an "assembly" process. "Assembly" is the bioinformatics process of combining reads form a de-novo genome sequencing project to form a "complete" genome (prokaryotes) or chromosome (eukaryotes). This is a difficult process! The biggest problem is that many parts of genomes are copies of each other or very similar, and the assembly software doesn't know which way to combine them (ambiguity). In fact, the human genome assembly though announced in may 2000 was only recently truly finished and published in Science after 22 years! Assembly software uses a number of parameters to try to combine the reads as well as possible into the largest unambigous DNA fragments, called "contigs". Assemblers tend to use graph-based algorithms (see this site for an excellent intro) and the most important parameter tuning those algorithms is the "kmer size". This specifies the amount of overlap that reads in the assembly graph are to have. The chief measure to determine if one assembly is "better" than the other is called "N50" which is the weighted median of the contigs produced; the N50 indicates that more than 50% of the total genome is contained in contigs larger than this number.
The goal is to write bash and python scripts which together try out the "velveth" assembler on a prokaryotic dataset. (Original article about Velvet and a hands-on guide on how to use it.). You should vary the "kmer" parameter within a reasonable range. Note that there are of course other programs which already use this strategy to "automatically" produce the best assembly, e.g. VelvetOptimiser and KMerGenie.

## 2. Deliverable
The deliverable is a Bash script called assignment4.sh and a Python script called assignment4.py in the Assignment4/ directory of your programming3 GitHub repository. (As always, pay attention to capitalization!).
The NGS data you should assemble is located at /data/dataprocessing/MinIONData/MG5267/ on the assemblix servers.
Your bash script should control the job; it should run velveth and velvetg, using GNU parallel of course to parallelize trying different kmer sizes. Your python script should examine the produced assemblies (they will be in FASTA format) and calculate the N50. Your python script should read any number of contigs from sys.stdin and write the N50 of those contigs to sys.stdout (as a simple number). GNU Parallel controls how many lines are sent into your python script using its chunking facilities, and using up to 16 jobs in parallel.
Your final output should be located in an "output/" directory and should be the "best" assembly FASTA file as measured by N50, as well as a CSV file containing the kmer parameters tried and the N50 of the resulting assembly. (Be sure to delete all the other assembly files produced by your script except the best one)
You may use SLURM if you wish, but be sure to limit the number of CPU's to 16 (see below), limit the time to 48 hours, and use only the assemblix SLURM partition! Using a simple tmux based environment works fine too.

- NB to keep the assemblix computers usable for everyone, use maximum job size of 16!
